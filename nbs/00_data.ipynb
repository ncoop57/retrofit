{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import torch\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data.dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class RetroDataset(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_name,\n",
    "        encoder_name,\n",
    "        encoder_tokenizer_name,\n",
    "        decoder_tokenizer_name,\n",
    "        dataset_config=None,\n",
    "        column=\"text\",\n",
    "        batch_size=32,\n",
    "        k=10,\n",
    "        n_perc=100\n",
    "    ):\n",
    "        self.dataset_name = dataset_name\n",
    "        self.column = column\n",
    "        self.encoder_name = encoder_name\n",
    "        self.encoder_tokenizer = AutoTokenizer.from_pretrained(encoder_tokenizer_name)\n",
    "        self.decoder_tokenizer = AutoTokenizer.from_pretrained(decoder_tokenizer_name)\n",
    "        self.decoder_tokenizer.pad_token = self.decoder_tokenizer.eos_token\n",
    "        self.dataset_config = dataset_config\n",
    "        self.batch_size = batch_size\n",
    "        self.k = k\n",
    "        self.n_perc = n_perc\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        # Download datasets and encoding model\n",
    "        self.model = SentenceTransformer(self.encoder_name)\n",
    "        self.knowledge_ds = load_dataset(self.dataset_name, self.dataset_config, split=f\"train[:{self.n_perc}%]\")\n",
    "        self.valid_ds = load_dataset(self.dataset_name, self.dataset_config, split=f\"validation[:{self.n_perc}%]\")\n",
    "\n",
    "        # Create knowledge embeddings for the retrieving examples\n",
    "        self.knowledge_ds = self.knowledge_ds.map(\n",
    "            lambda example: {\n",
    "                \"embedding\": self.model.encode(example[self.column])\n",
    "            },\n",
    "            batched=True\n",
    "        )\n",
    "        self.knowledge_ds.set_format(type=\"numpy\", columns=[\"embedding\"], output_all_columns=True)\n",
    "        self.knowledge_ds.add_faiss_index(column=\"embedding\")\n",
    "\n",
    "        # Encod the validation examples for the retrieval\n",
    "        self.valid_ds = self.valid_ds.map(\n",
    "            lambda example: {\n",
    "                \"embedding\": self.model.encode(example[self.column])\n",
    "            },\n",
    "            batched=True\n",
    "        )\n",
    "        self.valid_ds.set_format(type=\"numpy\", columns=[\"embedding\"], output_all_columns=True)\n",
    "\n",
    "        def get_nearest_neighbors(example):\n",
    "            # Get the nearest neighbors of the example and tokenize them for the encoder\n",
    "            _, retrieved_examples = self.knowledge_ds.get_nearest_examples(\"embedding\", example[\"embedding\"], k=self.k + 1)\n",
    "            retrieved_input = self.encoder_tokenizer.cls_token.join(retrieved_examples[self.column][1:])\n",
    "            output = self.encoder_tokenizer(retrieved_input, padding=\"max_length\", truncation=True)\n",
    "\n",
    "            return {\n",
    "                \"retrieved_input_ids\": output[\"input_ids\"],\n",
    "                \"retrieved_attention_mask\": output[\"attention_mask\"]\n",
    "            }\n",
    "        \n",
    "        # Create training and validation dataset with retrieved examples\n",
    "        self.train_ds = self.knowledge_ds.map(get_nearest_neighbors)\n",
    "        self.valid_ds = self.valid_ds.map(get_nearest_neighbors)\n",
    "\n",
    "        # Tokenize the labels for the decoder\n",
    "        self.train_ds = self.train_ds.map(\n",
    "            lambda examples: self.decoder_tokenizer(examples[self.column], padding=\"max_length\", truncation=True),\n",
    "            batched=True\n",
    "        )\n",
    "        self.valid_ds = self.valid_ds.map(\n",
    "            lambda examples: self.decoder_tokenizer(examples[self.column], padding=\"max_length\", truncation=True),\n",
    "            batched=True\n",
    "        )\n",
    "\n",
    "        # Set everything to torch tensors\n",
    "        self.train_ds.set_format(\n",
    "            type=\"torch\",\n",
    "            columns=[\"input_ids\", \"retrieved_input_ids\", \"attention_mask\", \"retrieved_attention_mask\"],\n",
    "        )\n",
    "        self.valid_ds.set_format(\n",
    "            type=\"torch\",\n",
    "            columns=[\"input_ids\", \"retrieved_input_ids\", \"attention_mask\", \"retrieved_attention_mask\"],\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_ds, batch_size=self.batch_size, shuffle=True)\n",
    "    \n",
    "    def valid_dataloader(self):\n",
    "        return DataLoader(self.valid_ds, batch_size=self.batch_size, shuffle=False)\n",
    "    \n",
    "    def get_nearest_neighbors(self, example, k=10):\n",
    "        embed = self.model.encode(example)\n",
    "        _, retrieved_examples = self.knowledge_ds.get_nearest_examples(\"embeddings\", embed, k=k)\n",
    "\n",
    "        return retrieved_examples[self.column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_data.ipynb.\n",
      "Converted 01_model.ipynb.\n",
      "Converted 02_tutorial.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script\n",
    "\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
