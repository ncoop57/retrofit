{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch.utils.data.dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class RetroDataset(pl.LightningDataModule):\n",
    "    def __init__(self, dataset_name, column, encoder_name, dataset_config=None, batch_size=32, k=10, n_perc=100):\n",
    "        self.dataset_name = dataset_name\n",
    "        self.column = column\n",
    "        self.encoder_name = encoder_name\n",
    "        self.dataset_config = dataset_config\n",
    "        self.batch_size = batch_size\n",
    "        self.k = k\n",
    "        self.n_perc = n_perc\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        self.model = SentenceTransformer(self.encoder_name)\n",
    "        train_ds = load_dataset(self.dataset_name, self.dataset_config, split=f\"train[:{self.n_perc}]\")\n",
    "        valid_ds = load_dataset(self.dataset_name, self.dataset_config, split=f\"validation[:{self.n_perc}]\")\n",
    "\n",
    "        train_ds = train_ds.map(lambda example: {\"embeddings\": self.model.encode(example[self.column])}, batched=True)\n",
    "        train_ds.add_faiss_index(column=\"embeddings\")\n",
    "        valid_ds = valid_ds.map(lambda example: {\"embeddings\": self.model.encode(example[self.column])}, batched=True)\n",
    "        valid_ds.add_faiss_index(column=\"embeddings\")\n",
    "\n",
    "        def get_nearest_neighbors(example):\n",
    "            _, retrieved_examples = train_ds.get_nearest_examples(\"embeddings\", example[\"embeddings\"], k=self.k)\n",
    "            example[\"retrieved_examples\"] = retrieved_examples[self.column]\n",
    "\n",
    "            return example\n",
    "        \n",
    "        self.train_ds = train_ds.map(get_nearest_neighbors)\n",
    "        self.valid_ds = valid_ds.map(get_nearest_neighbors)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_ds, batch_size=self.batch_size, shuffle=True)\n",
    "    \n",
    "    def valid_dataloader(self):\n",
    "        return DataLoader(self.valid_ds, batch_size=self.batch_size, shuffle=True)\n",
    "    \n",
    "    def get_nearest_neighbors(self, example, k=10):\n",
    "        embed = self.model.encode(example)\n",
    "        _, retrieved_examples = self.train_ds.get_nearest_examples(\"embeddings\", embed, k=k)\n",
    "\n",
    "        return retrieved_examples[self.column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script\n",
    "\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
