# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/00_data.ipynb (unless otherwise specified).

__all__ = ['RetroDataset']

# Cell
import torch

import pytorch_lightning as pl

from datasets import load_dataset
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer
from torch.utils.data.dataloader import DataLoader

# Cell
class RetroDataset(pl.LightningDataModule):
    def __init__(
        self,
        dataset_name,
        encoder_name,
        encoder_tokenizer_name,
        decoder_tokenizer_name,
        dataset_config=None,
        column="text",
        batch_size=32,
        k=10,
        n_perc=100
    ):
        self.dataset_name = dataset_name
        self.column = column
        self.encoder_name = encoder_name
        self.encoder_tokenizer = AutoTokenizer.from_pretrained(encoder_tokenizer_name)
        self.decoder_tokenizer = AutoTokenizer.from_pretrained(decoder_tokenizer_name)
        self.decoder_tokenizer.pad_token = self.decoder_tokenizer.eos_token
        self.dataset_config = dataset_config
        self.batch_size = batch_size
        self.k = k
        self.n_perc = n_perc

    def setup(self, stage=None):
        self.model = SentenceTransformer(self.encoder_name)
        self.knowledge_ds = load_dataset(self.dataset_name, self.dataset_config, split=f"train[:{self.n_perc}%]")
        self.valid_ds = load_dataset(self.dataset_name, self.dataset_config, split=f"validation[:{self.n_perc}%]")

        # def process_ds(ds):
        #     ds = ds.map(
        #         lambda examples: self.decoder_tokenizer(examples[self.column], padding="max_length", truncation=True),
        #         batched=True
        #     )
        #     ds = ds.map(
        #         lambda example: {
        #             "embedding": self.model.encode(example[self.column])
        #         },
        #         batched=True
        #     )
        #     ds.set_format(type="numpy", columns=["embedding"], output_all_columns=True)
        #     # train_ds.add_faiss_index(column="embedding")
        #     return ds

        # self.train_ds = process_ds(self.train_ds)
        # self.train_ds.add_faiss_index(column="embedding")
        # self.valid_ds = process_ds(self.valid_ds)
        # self.valid_ds.add_faiss_index(column="embedding")



        self.knowledge_ds = self.knowledge_ds.map(
            lambda example: {
                "embedding": self.model.encode(example[self.column])#, convert_to_tensor=True)
            },
            batched=True
        )
        self.knowledge_ds.set_format(type="numpy", columns=["embedding"], output_all_columns=True)
        self.knowledge_ds.add_faiss_index(column="embedding")
        self.valid_ds = self.valid_ds.map(
            lambda example: {
                "embedding": self.model.encode(example[self.column])#, convert_to_tensor=True)
            },
            batched=True
        )
        self.valid_ds.set_format(type="numpy", columns=["embedding"], output_all_columns=True)
        # self.valid_ds.add_faiss_index(column="embedding")

        def get_nearest_neighbors(example):
            _, retrieved_examples = self.knowledge_ds.get_nearest_examples("embedding", example["embedding"], k=self.k + 1)
            retrieved_input = self.encoder_tokenizer.cls_token.join(retrieved_examples[self.column][1:])
            output = self.encoder_tokenizer(retrieved_input, padding="max_length", truncation=True)

            return {
                "retrieved_input_ids": output["input_ids"],
                "retrieved_attention_mask": output["attention_mask"]
            }

        self.train_ds = self.knowledge_ds.map(get_nearest_neighbors)
        self.valid_ds = self.valid_ds.map(get_nearest_neighbors)

        self.train_ds = self.train_ds.map(
            lambda examples: self.decoder_tokenizer(examples[self.column], padding="max_length", truncation=True),
            batched=True
        )
        self.valid_ds = self.valid_ds.map(
            lambda examples: self.decoder_tokenizer(examples[self.column], padding="max_length", truncation=True),
            batched=True
        )

        self.train_ds.set_format(
            type="torch",
            columns=["input_ids", "retrieved_input_ids", "attention_mask", "retrieved_attention_mask"],
        )
        self.valid_ds.set_format(
            type="torch",
            columns=["input_ids", "retrieved_input_ids", "attention_mask", "retrieved_attention_mask"],
        )

    def collate_fn(self, batch):
        print(batch)
        return batch

    def train_dataloader(self):
        return DataLoader(self.train_ds, batch_size=self.batch_size, shuffle=True)

    def valid_dataloader(self):
        return DataLoader(self.valid_ds, batch_size=self.batch_size, shuffle=False)

    def get_nearest_neighbors(self, example, k=10):
        embed = self.model.encode(example)
        _, retrieved_examples = self.knowledge_ds.get_nearest_examples("embeddings", embed, k=k)

        return retrieved_examples[self.column]