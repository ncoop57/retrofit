---

title: Tutorial


keywords: fastai
sidebar: home_sidebar

summary: "API details."
description: "API details."
nb_path: "nbs/02_tutorial.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/02_tutorial.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">wandb</span>

<span class="kn">import</span> <span class="nn">pytorch_lightning</span> <span class="k">as</span> <span class="nn">pl</span>

<span class="c1"># from codecarbon import EmissionsTracker</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="k">import</span> <span class="n">Path</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.callbacks</span> <span class="k">import</span> <span class="n">EarlyStopping</span><span class="p">,</span> <span class="n">ModelCheckpoint</span><span class="p">,</span> <span class="n">TQDMProgressBar</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.loggers</span> <span class="k">import</span> <span class="n">WandbLogger</span>
<span class="kn">from</span> <span class="nn">retrofit.data</span> <span class="k">import</span> <span class="n">RetroDataset</span>
<span class="kn">from</span> <span class="nn">retrofit.model</span> <span class="k">import</span> <span class="n">RetroFitModelWrapper</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="k">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">EncoderDecoderModel</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">column</span> <span class="o">=</span> <span class="s2">&quot;whole_func_string&quot;</span>
<span class="n">encoder_name</span> <span class="o">=</span> <span class="s2">&quot;distilbert-base-uncased&quot;</span>
<span class="n">decoder_name</span> <span class="o">=</span> <span class="s2">&quot;gpt2&quot;</span>
<span class="n">encoder_tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">encoder_name</span><span class="p">)</span>
<span class="n">decoder_tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">decoder_name</span><span class="p">)</span>
<span class="n">decoder_tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">decoder_tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
<span class="n">retro_ds</span> <span class="o">=</span> <span class="n">RetroDataset</span><span class="p">(</span>
    <span class="s2">&quot;code_search_net&quot;</span><span class="p">,</span>
    <span class="s2">&quot;flax-sentence-embeddings/st-codesearch-distilroberta-base&quot;</span><span class="p">,</span>
    <span class="n">encoder_tokenizer</span><span class="p">,</span>
    <span class="n">decoder_tokenizer</span><span class="p">,</span>
    <span class="n">dataset_config</span><span class="o">=</span><span class="s2">&quot;python&quot;</span><span class="p">,</span>
    <span class="n">column</span><span class="o">=</span><span class="n">column</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">k</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">n_perc</span><span class="o">=</span><span class="mi">1</span>
<span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">EncoderDecoderModel</span><span class="o">.</span><span class="n">from_encoder_decoder_pretrained</span><span class="p">(</span><span class="n">encoder_name</span><span class="p">,</span> <span class="n">decoder_name</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">decoder_start_token_id</span> <span class="o">=</span> <span class="n">decoder_tokenizer</span><span class="o">.</span><span class="n">bos_token_id</span>
<span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">decoder_tokenizer</span><span class="o">.</span><span class="n">pad_token_id</span>
<span class="n">retro_model</span> <span class="o">=</span> <span class="n">RetroFitModelWrapper</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">lr</span><span class="o">=</span><span class="mf">5e-4</span><span class="p">,</span>
    <span class="n">freeze_decoder</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: [&#39;vocab_layer_norm.bias&#39;, &#39;vocab_layer_norm.weight&#39;, &#39;vocab_transform.weight&#39;, &#39;vocab_transform.bias&#39;, &#39;vocab_projector.weight&#39;, &#39;vocab_projector.bias&#39;]
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: [&#39;h.2.crossattention.c_attn.weight&#39;, &#39;h.7.crossattention.c_attn.weight&#39;, &#39;h.0.crossattention.c_attn.weight&#39;, &#39;h.8.crossattention.masked_bias&#39;, &#39;h.9.ln_cross_attn.weight&#39;, &#39;h.8.crossattention.q_attn.weight&#39;, &#39;h.6.crossattention.c_proj.weight&#39;, &#39;h.11.crossattention.c_proj.bias&#39;, &#39;h.9.crossattention.c_proj.bias&#39;, &#39;h.3.crossattention.c_proj.bias&#39;, &#39;h.8.crossattention.c_proj.bias&#39;, &#39;h.2.crossattention.c_proj.weight&#39;, &#39;h.3.crossattention.c_proj.weight&#39;, &#39;h.11.ln_cross_attn.weight&#39;, &#39;h.1.crossattention.bias&#39;, &#39;h.4.crossattention.c_attn.weight&#39;, &#39;h.7.crossattention.q_attn.weight&#39;, &#39;h.2.crossattention.c_proj.bias&#39;, &#39;h.10.crossattention.bias&#39;, &#39;h.7.crossattention.masked_bias&#39;, &#39;h.1.crossattention.masked_bias&#39;, &#39;h.7.crossattention.c_proj.bias&#39;, &#39;h.0.ln_cross_attn.weight&#39;, &#39;h.6.crossattention.c_attn.weight&#39;, &#39;h.1.ln_cross_attn.weight&#39;, &#39;h.2.crossattention.q_attn.weight&#39;, &#39;h.4.crossattention.masked_bias&#39;, &#39;h.6.crossattention.masked_bias&#39;, &#39;h.11.crossattention.c_attn.weight&#39;, &#39;h.8.crossattention.c_attn.weight&#39;, &#39;h.10.ln_cross_attn.weight&#39;, &#39;h.8.crossattention.c_proj.weight&#39;, &#39;h.10.crossattention.masked_bias&#39;, &#39;h.5.crossattention.q_attn.weight&#39;, &#39;h.0.crossattention.masked_bias&#39;, &#39;h.0.crossattention.c_proj.weight&#39;, &#39;h.10.crossattention.c_attn.weight&#39;, &#39;h.4.crossattention.q_attn.weight&#39;, &#39;h.8.ln_cross_attn.weight&#39;, &#39;h.5.crossattention.c_attn.weight&#39;, &#39;h.5.crossattention.masked_bias&#39;, &#39;h.7.ln_cross_attn.weight&#39;, &#39;h.9.crossattention.c_attn.weight&#39;, &#39;h.0.crossattention.bias&#39;, &#39;h.3.crossattention.q_attn.weight&#39;, &#39;h.6.ln_cross_attn.weight&#39;, &#39;h.1.crossattention.c_proj.bias&#39;, &#39;h.6.crossattention.q_attn.weight&#39;, &#39;h.7.crossattention.c_proj.weight&#39;, &#39;h.11.crossattention.c_proj.weight&#39;, &#39;h.9.crossattention.c_proj.weight&#39;, &#39;h.7.crossattention.bias&#39;, &#39;h.3.crossattention.c_attn.weight&#39;, &#39;h.3.crossattention.bias&#39;, &#39;h.0.crossattention.c_proj.bias&#39;, &#39;h.2.crossattention.bias&#39;, &#39;h.2.crossattention.masked_bias&#39;, &#39;h.11.crossattention.bias&#39;, &#39;h.10.crossattention.c_proj.bias&#39;, &#39;h.10.crossattention.c_proj.weight&#39;, &#39;h.5.crossattention.c_proj.bias&#39;, &#39;h.6.crossattention.bias&#39;, &#39;h.10.crossattention.q_attn.weight&#39;, &#39;h.11.crossattention.q_attn.weight&#39;, &#39;h.9.crossattention.masked_bias&#39;, &#39;h.11.crossattention.masked_bias&#39;, &#39;h.4.ln_cross_attn.weight&#39;, &#39;h.4.crossattention.c_proj.bias&#39;, &#39;h.4.crossattention.c_proj.weight&#39;, &#39;h.1.crossattention.c_attn.weight&#39;, &#39;h.9.crossattention.q_attn.weight&#39;, &#39;h.2.ln_cross_attn.weight&#39;, &#39;h.5.ln_cross_attn.weight&#39;, &#39;h.3.crossattention.masked_bias&#39;, &#39;h.1.crossattention.q_attn.weight&#39;, &#39;h.5.crossattention.c_proj.weight&#39;, &#39;h.1.crossattention.c_proj.weight&#39;, &#39;h.9.crossattention.bias&#39;, &#39;h.8.crossattention.bias&#39;, &#39;h.0.crossattention.q_attn.weight&#39;, &#39;h.5.crossattention.bias&#39;, &#39;h.4.crossattention.bias&#39;, &#39;h.6.crossattention.c_proj.bias&#39;, &#39;h.3.ln_cross_attn.weight&#39;]
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">train_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">data_module</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">,</span> <span class="n">output_dir</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Train a model with a given training data loader, validation data loader,</span>
<span class="sd">    optimizer, scheduler, loss function, metrics, and callbacks.</span>

<span class="sd">    Args:</span>
<span class="sd">        model (pl.LightningModule): The model to train.</span>
<span class="sd">        data_module (pl.LightningDataModule): The data module to use for training.</span>
<span class="sd">        num_epochs (int): The number of epochs to train for.</span>
<span class="sd">        output_dir (pathlib.Path): The directory to save the model to.</span>
<span class="sd">        name (str): The name of the model.</span>
<span class="sd">    Returns:</span>
<span class="sd">        best_model_path (str): The path to the best model&#39;s checkpoint.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># pl.seed_everything(115, workers=True)</span>
    <span class="n">wandb_logger</span> <span class="o">=</span> <span class="n">WandbLogger</span><span class="p">(</span><span class="n">project</span><span class="o">=</span><span class="s2">&quot;Retrofit&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
    <span class="c1"># saves a file like: my/path/sample-mnist-epoch=02-val_loss=0.32.ckpt</span>
    <span class="n">checkpoint_callback</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">(</span>
        <span class="n">monitor</span><span class="o">=</span><span class="s2">&quot;val_loss&quot;</span><span class="p">,</span>
        <span class="n">dirpath</span><span class="o">=</span><span class="nb">str</span><span class="p">(</span><span class="n">output_dir</span> <span class="o">/</span> <span class="s2">&quot;checkpoints&quot;</span><span class="p">),</span>
        <span class="n">filename</span><span class="o">=</span><span class="s2">&quot;retrofit-</span><span class="si">{epoch:02d}</span><span class="s2">-</span><span class="si">{val_loss:.2f}</span><span class="s2">&quot;</span><span class="p">,</span>
        <span class="n">save_top_k</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
        <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;min&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">trainer</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">Trainer</span><span class="p">(</span>
        <span class="n">logger</span><span class="o">=</span><span class="n">wandb_logger</span><span class="p">,</span>
        <span class="n">default_root_dir</span><span class="o">=</span><span class="nb">str</span><span class="p">(</span><span class="n">output_dir</span> <span class="o">/</span> <span class="s2">&quot;checkpoints&quot;</span><span class="p">),</span>
        <span class="n">gpus</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">(),</span>
        <span class="n">max_epochs</span><span class="o">=</span><span class="n">num_epochs</span><span class="p">,</span>
        <span class="n">limit_train_batches</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
        <span class="n">limit_val_batches</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
        <span class="n">precision</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
        <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span>
            <span class="n">checkpoint_callback</span><span class="p">,</span>
            <span class="n">EarlyStopping</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s2">&quot;val_loss&quot;</span><span class="p">),</span>
            <span class="n">TQDMProgressBar</span><span class="p">(</span><span class="n">refresh_rate</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
        <span class="p">],</span>
    <span class="p">)</span>
    <span class="c1"># tracker = EmissionsTracker(output_dir=output_dir.parent.parent, project_name=name)</span>

    <span class="c1"># train the model and track emissions</span>
    <span class="c1"># tracker.start()</span>
    <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">data_module</span><span class="p">)</span>
    <span class="c1"># tracker.stop()</span>

    <span class="c1"># save the best model to wandb</span>
    <span class="n">best_model_path</span> <span class="o">=</span> <span class="n">checkpoint_callback</span><span class="o">.</span><span class="n">best_model_path</span>
    <span class="k">if</span> <span class="n">best_model_path</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">wandb</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">best_model_path</span><span class="p">)</span>

    <span class="c1"># # save the emissions csv file</span>
    <span class="c1"># wandb.save(str(output_dir.parent.parent / &quot;emissions.csv&quot;))</span>

    <span class="k">return</span> <span class="n">best_model_path</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">out_dir</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s2">&quot;/workspace/retrofit/data/output/&quot;</span><span class="p">)</span>
<span class="n">best_model_path</span> <span class="o">=</span> <span class="n">train_model</span><span class="p">(</span>
    <span class="n">retro_model</span><span class="p">,</span>
    <span class="n">retro_ds</span><span class="p">,</span>
    <span class="n">num_epochs</span><span class="o">=</span><span class="n">num_epochs</span><span class="p">,</span>
    <span class="n">output_dir</span><span class="o">=</span><span class="n">out_dir</span> <span class="o">/</span> <span class="s2">&quot;model&quot;</span><span class="p">,</span>
    <span class="n">name</span><span class="o">=</span><span class="s2">&quot;test&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="c1"># model = RetroFitModel.load_from_checkpoint(best_model_path)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>Using 16bit native Automatic Mixed Precision (AMP)
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
Reusing dataset code_search_net (/root/.cache/huggingface/datasets/code_search_net/python/1.0.0/80a244ab541c6b2125350b764dc5c2b715f65f00de7a56107a28915fac173a27)
Reusing dataset code_search_net (/root/.cache/huggingface/datasets/code_search_net/python/1.0.0/80a244ab541c6b2125350b764dc5c2b715f65f00de7a56107a28915fac173a27)
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.
<span class="ansi-blue-intense-fg ansi-bold">wandb</span>: Currently logged in as: <span class="ansi-yellow-fg">natedog</span> (use `wandb login --relogin` to force relogin)
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

                    Syncing run <strong><a href="https://wandb.ai/natedog/Retrofit/runs/2nkmicpv" target="_blank">test</a></strong> to <a href="https://wandb.ai/natedog/Retrofit" target="_blank">Weights & Biases</a> (<a href="https://docs.wandb.com/integrations/jupyter.html" target="_blank">docs</a>).<br/>

                
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>
  | Name  | Type                | Params
----------------------------------------------
0 | model | EncoderDecoderModel | 219 M 
----------------------------------------------
219 M     Trainable params
0         Non-trainable params
219 M     Total params
438.339   Total estimated model params size (MB)
/usr/local/lib/python3.8/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:617: UserWarning: Checkpoint directory /workspace/retrofit/data/output/model/checkpoints exists and is not empty.
  rank_zero_warn(f&#34;Checkpoint directory {dirpath} exists and is not empty.&#34;)
/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/data_loading.py:110: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/usr/local/lib/python3.8/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:527: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/data_loading.py:110: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
<span class="ansi-blue-intense-fg ansi-bold">wandb</span>: <span class="ansi-yellow-fg">WARNING</span> Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(&#34;/mnt/folder/file.h5&#34;, base_path=&#34;/mnt&#34;)
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

